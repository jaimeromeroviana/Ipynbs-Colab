{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simulator_MEC.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaimeromeroviana/Ipynbs-Colab/blob/main/Simulator_MEC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UujBi7b6vd4T"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "import cv2\n",
        "from keras.models import load_model, Sequential\n",
        "from keras.layers.convolutional import Convolution2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.core import Activation, Dropout, Flatten, Dense\n",
        "from keras import backend as K\n",
        "\n",
        "from collections import deque"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rrNQI5nwfU8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r00fwgTxfAW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "a38e08cb-7a0c-40e9-a9f8-417bd9db5fca"
      },
      "source": [
        "# List of hyper-parameters and constants\n",
        "DECAY_RATE = 0.99\n",
        "BUFFER_SIZE = 40000\n",
        "MINIBATCH_SIZE = 64\n",
        "TOT_FRAME = 3000000\n",
        "EPSILON_DECAY = 1000000\n",
        "MIN_OBSERVATION = 5000\n",
        "FINAL_EPSILON = 0.05\n",
        "INITIAL_EPSILON = 0.1\n",
        "NUM_ACTIONS = 6\n",
        "TAU = 0.01\n",
        "# Number of frames to throw into network\n",
        "NUM_FRAMES = 3\n",
        "\n",
        "class DeepQ(object):\n",
        "    \"\"\"Constructs the desired deep q learning network\"\"\"\n",
        "    def __init__(self):\n",
        "        self.construct_q_network()\n",
        "\n",
        "    def construct_q_network(self):\n",
        "        # Uses the network architecture found in DeepMind paper\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Convolution2D(32, 8, 8, subsample=(4, 4), input_shape=(84, 84, NUM_FRAMES)))\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(Convolution2D(64, 4, 4, subsample=(2, 2)))\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(Convolution2D(64, 3, 3))\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(Flatten())\n",
        "        self.model.add(Dense(512))\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(Dense(NUM_ACTIONS))\n",
        "        self.model.compile(loss='mse', optimizer=Adam(lr=0.00001))\n",
        "\n",
        "        # Creates a target network as described in DeepMind paper\n",
        "        self.target_model = Sequential()\n",
        "        self.target_model.add(Convolution2D(32, 8, 8, subsample=(4, 4), input_shape=(84, 84, NUM_FRAMES)))\n",
        "        self.target_model.add(Activation('relu'))\n",
        "        self.target_model.add(Convolution2D(64, 4, 4, subsample=(2, 2)))\n",
        "        self.target_model.add(Activation('relu'))\n",
        "        self.target_model.add(Convolution2D(64, 3, 3))\n",
        "        self.target_model.add(Activation('relu'))\n",
        "        self.target_model.add(Flatten())\n",
        "        self.target_model.add(Dense(512))\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.target_model.add(Dense(NUM_ACTIONS))\n",
        "        self.target_model.compile(loss='mse', optimizer=Adam(lr=0.00001))\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        print(\"Successfully constructed networks.\")\n",
        "\n",
        "    def predict_movement(self, data, epsilon):\n",
        "        \"\"\"Predict movement of game controler where is epsilon\n",
        "        probability randomly move.\"\"\"\n",
        "        q_actions = self.model.predict(data.reshape(1, 84, 84, NUM_FRAMES), batch_size = 1)\n",
        "        opt_policy = np.argmax(q_actions)\n",
        "        rand_val = np.random.random()\n",
        "        if rand_val < epsilon:\n",
        "            opt_policy = np.random.randint(0, NUM_ACTIONS)\n",
        "        return opt_policy, q_actions[0, opt_policy]\n",
        "\n",
        "    def train(self, s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num):\n",
        "        \"\"\"Trains network to fit given parameters\"\"\"\n",
        "        batch_size = s_batch.shape[0]\n",
        "        targets = np.zeros((batch_size, NUM_ACTIONS))\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            targets[i] = self.model.predict(s_batch[i].reshape(1, 84, 84, NUM_FRAMES), batch_size = 1)\n",
        "            fut_action = self.target_model.predict(s2_batch[i].reshape(1, 84, 84, NUM_FRAMES), batch_size = 1)\n",
        "            targets[i, a_batch[i]] = r_batch[i]\n",
        "            if d_batch[i] == False:\n",
        "                targets[i, a_batch[i]] += DECAY_RATE * np.max(fut_action)\n",
        "\n",
        "        loss = self.model.train_on_batch(s_batch, targets)\n",
        "\n",
        "        # Print the loss every 10 iterations.\n",
        "        if observation_num % 10 == 0:\n",
        "            print(\"We had a loss equal to \", loss)\n",
        "\n",
        "\n",
        "    def target_train(self):\n",
        "        model_weights = self.model.get_weights()\n",
        "        target_model_weights = self.target_model.get_weights()\n",
        "        for i in range(len(model_weights)):\n",
        "            target_model_weights[i] = TAU * model_weights[i] + (1 - TAU) * target_model_weights[i]\n",
        "        self.target_model.set_weights(target_model_weights)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Haven't finished implementing yet...'\")\n",
        "    space_invader = SpaceInvader()\n",
        "    # print space_invader.calculate_mean()\n",
        "    # space_invader.simulate(\"deep_q_video\", True)\n",
        "    space_invader.train(TOT_FRAME)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Haven't finished implementing yet...'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c116902308c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Haven't finished implementing yet...'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mspace_invader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpaceInvader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;31m# print space_invader.calculate_mean()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# space_invader.simulate(\"deep_q_video\", True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'mode'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mlmmBaPwfP1"
      },
      "source": [
        "# List of hyper-parameters and constants\n",
        "DECAY_RATE = 0.99\n",
        "NUM_ACTIONS = 6\n",
        "# Number of frames to throw into network\n",
        "NUM_FRAMES = 3\n",
        "\n",
        "class DuelQ(object):\n",
        "    \"\"\"Constructs the desired deep q learning network\"\"\"\n",
        "    def __init__(self):\n",
        "        self.construct_q_network()\n",
        "\n",
        "    def construct_q_network(self):\n",
        "        # Uses the network architecture found in DeepMind paper\n",
        "        self.model = Sequential()\n",
        "        input_layer = Input(shape = (84, 84, NUM_FRAMES))\n",
        "        conv1 = Convolution2D(32, 8, 8, subsample=(4, 4), activation='relu')(input_layer)\n",
        "        conv2 = Convolution2D(64, 4, 4, subsample=(2, 2), activation='relu')(conv1)\n",
        "        conv3 = Convolution2D(64, 3, 3, activation = 'relu')(conv2)\n",
        "        flatten = Flatten()(conv3)\n",
        "        fc1 = Dense(512)(flatten)\n",
        "        advantage = Dense(NUM_ACTIONS)(fc1)\n",
        "        fc2 = Dense(512)(flatten)\n",
        "        value = Dense(1)(fc2)\n",
        "        policy = merge([advantage, value], mode = lambda x: x[0]-K.mean(x[0])+x[1], output_shape = (NUM_ACTIONS,))\n",
        "        # policy = Dense(NUM_ACTIONS)(merge_layer)\n",
        "\n",
        "        self.model = Model(input=[input_layer], output=[policy])\n",
        "        self.model.compile(loss='mse', optimizer=Adam(lr=0.000001))\n",
        "\n",
        "        self.target_model = Model(input=[input_layer], output=[policy])\n",
        "        self.target_model.compile(loss='mse', optimizer=Adam(lr=0.000001))\n",
        "        print(\"Successfully constructed networks.\")\n",
        "\n",
        "    def predict_movement(self, data, epsilon):\n",
        "        \"\"\"Predict movement of game controler where is epsilon\n",
        "        probability randomly move.\"\"\"\n",
        "        q_actions = self.model.predict(data.reshape(1, 84, 84, NUM_FRAMES), batch_size = 1)\n",
        "        opt_policy = np.argmax(q_actions)\n",
        "        rand_val = np.random.random()\n",
        "        if rand_val < epsilon:\n",
        "            opt_policy = np.random.randint(0, NUM_ACTIONS)\n",
        "        return opt_policy, q_actions[0, opt_policy]\n",
        "\n",
        "    def train(self, s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num):\n",
        "        \"\"\"Trains network to fit given parameters\"\"\"\n",
        "        batch_size = s_batch.shape[0]\n",
        "        targets = np.zeros((batch_size, NUM_ACTIONS))\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            targets[i] = self.model.predict(s_batch[i].reshape(1, 84, 84, NUM_FRAMES), batch_size = 1)\n",
        "            fut_action = self.target_model.predict(s2_batch[i].reshape(1, 84, 84, NUM_FRAMES), batch_size = 1)\n",
        "            targets[i, a_batch[i]] = r_batch[i]\n",
        "            if d_batch[i] == False:\n",
        "                targets[i, a_batch[i]] += DECAY_RATE * np.max(fut_action)\n",
        "\n",
        "        loss = self.model.train_on_batch(s_batch, targets)\n",
        "\n",
        "        # Print the loss every 10 iterations.\n",
        "        if observation_num % 10 == 0:\n",
        "            print(\"We had a loss equal to \", loss)\n",
        "\n",
        "    def save_network(self, path):\n",
        "        # Saves model at specified path as h5 file\n",
        "        self.model.save(path)\n",
        "        print(\"Successfully saved network.\")\n",
        "\n",
        "    def load_network(self, path):\n",
        "        self.model.load_weights(path)\n",
        "        self.target_model.load_weights(path)\n",
        "        print(\"Succesfully loaded network.\")\n",
        "\n",
        "    def target_train(self):\n",
        "        model_weights = self.model.get_weights()\n",
        "        self.target_model.set_weights(model_weights)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Haven't finished implementing yet...'\")\n",
        "    space_invader = SpaceInvader()\n",
        "    space_invader.load_network(\"duel_saved.h5\")\n",
        "    # print space_invader.calculate_mean()\n",
        "    space_invader.simulate(\"duel_q_video_2\", True)\n",
        "    # space_invader.train(TOT_FRAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF0sXEWZwfSk"
      },
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Constructs a buffer object that stores the past moves\n",
        "    and samples a set of subsamples\"\"\"\n",
        "\n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.count = 0\n",
        "        self.buffer = deque()\n",
        "\n",
        "    def add(self, s, a, r, d, s2):\n",
        "        \"\"\"Add an experience to the buffer\"\"\"\n",
        "        # S represents current state, a is action,\n",
        "        # r is reward, d is whether it is the end, \n",
        "        # and s2 is next state\n",
        "        experience = (s, a, r, d, s2)\n",
        "        if self.count < self.buffer_size:\n",
        "            self.buffer.append(experience)\n",
        "            self.count += 1\n",
        "        else:\n",
        "            self.buffer.popleft()\n",
        "            self.buffer.append(experience)\n",
        "\n",
        "    def size(self):\n",
        "        return self.count\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Samples a total of elements equal to batch_size from buffer\n",
        "        if buffer contains enough elements. Otherwise return all elements\"\"\"\n",
        "\n",
        "        batch = []\n",
        "\n",
        "        if self.count < batch_size:\n",
        "            batch = random.sample(self.buffer, self.count)\n",
        "        else:\n",
        "            batch = random.sample(self.buffer, batch_size)\n",
        "\n",
        "        # Maps each experience in batch in batches of states, actions, rewards\n",
        "        # and new states\n",
        "        s_batch, a_batch, r_batch, d_batch, s2_batch = list(map(np.array, list(zip(*batch))))\n",
        "\n",
        "        return s_batch, a_batch, r_batch, d_batch, s2_batch\n",
        "\n",
        "    def clear(self):\n",
        "        self.buffer.clear()\n",
        "        self.count = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn1fnQ0cyo1h"
      },
      "source": [
        "# Hyperparameters\n",
        "NUM_FRAME = 1000000\n",
        "\n",
        "\n",
        "game_instance = SpaceInvader(args.network)\n",
        "\n",
        "game_instance.load_network(args.load)\n",
        "\n",
        "game_instance.train(NUM_FRAME)\n",
        "\n",
        "game_instance.simulate()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}