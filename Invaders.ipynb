{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Invaders.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "17nRcBW4u3n7RF-uAIwh7Mam5V-d7qow7",
      "authorship_tag": "ABX9TyN8ifUOspPWj1v+WVqgsMEv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaimeromeroviana/Ipynbs-Colab/blob/main/Invaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI4AuiwO5ByQ"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive \n",
        "from google.colab import auth \n",
        "from oauth2client.client import GoogleCredentials\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTwkeYhv-kdY"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()                       \n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AolzmbgYNDG1",
        "outputId": "364b8421-35ff-4acb-f838-c7fe3c4c1259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "import numpy as np\n",
        "x = 0\n",
        "y = 0\n",
        "batch=np.zeros((3,4))\n",
        "print (batch)\n",
        "for i in range(5):\n",
        "  if(i==0):\n",
        "    x= x+1\n",
        "    y = 2*x\n",
        "    batch = np.array()\n",
        "    print (batch)\n",
        "  else:\n",
        "    x= x+1\n",
        "    y = 2*x\n",
        "    batch = np.insert(batch, [i], [[x],[y],[i],[4]], axis=1)\n",
        "    print(batch)\n",
        "\n",
        "      for j in range(minutos):\n",
        "    batch_predict=np.zeros((3,4))\n",
        "    if(j==775):\n",
        "      x= x+1\n",
        "      y = 2*x\n",
        "      batch_predict = np.array([[x], [y], [i], [j]])\n",
        "    elif(j>775 && j<785):\n",
        "      x= x+1\n",
        "      y = 2*x\n",
        "      batch_predict = np.insert(batch, [i], [[x],[y],[i],[j]], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"son las %d\", hour)\n",
        "    \n",
        "    q_list = model.predict(day,hour) #np array con los parámetros\n",
        "    message = np.argmax(q_list)\n",
        "    reward = step(message, hour)\n",
        "\n",
        "    newrow = [day,hour,message,reward,hour+1]\n",
        "    batch_train = np.vstack([batch_train, newrow])\n",
        "\n",
        "\n",
        "    if(hour%4=0):\n",
        "\n",
        "      states=0\n",
        "      future_states=0\n",
        "      qs_list = self.model.predict(states)\n",
        "      future_qs_list = self.target_model.predict(future_states)\n",
        "\n",
        "        X = []\n",
        "        Y = []\n",
        "\n",
        "\n",
        "        for index, (current_state, reward, future_states) in enumerate(batch_train):\n",
        "\n",
        "            if not done:\n",
        "                max_future_q = np.max(future_qs_list[index])\n",
        "                new_q = reward + DISCOUNT * max_future_q\n",
        "            else:\n",
        "                new_q = reward\n",
        "\n",
        "            current_qs = current_qs_list[index]\n",
        "            current_qs[action] = new_q\n",
        "\n",
        "            X.append(current_state)\n",
        "            y.append(current_qs)\n",
        "\n",
        "        self.model.fit(np.array(X)/255, np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False, callbacks=[self.tensorboard] if terminal_state else None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d4b03c64663a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Required argument 'object' (pos 1) not found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36uHKlYajaDD"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    \"\"\"Constructs a buffer object that stores the past moves\n",
        "    and samples a set of subsamples\"\"\"\n",
        "\n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.count = 0\n",
        "        self.buffer = deque()\n",
        "\n",
        "    def get_length(self):\n",
        "        return len(self.buffer)\n",
        "        \n",
        "    def add(self, state, action, reward, done, state2):\n",
        "        \"\"\"Add an experience to the buffer\"\"\"\n",
        "        # S represents current state, a is action,\n",
        "        # r is reward, d is whether it is the end, \n",
        "        # and s2 is next state\n",
        "        experience = (state, action, reward, done, state2)\n",
        "        if self.count < self.buffer_size:\n",
        "            self.buffer.append(experience)\n",
        "            self.count += 1\n",
        "        else:\n",
        "            self.buffer.popleft()\n",
        "            self.buffer.append(experience)\n",
        "\n",
        "    def set_penalty(self, position, PENALTY=-20):\n",
        "        #self.buffer[position][2] = PENALTY\n",
        "        #coges la posicion que quieres, conviertes a lista, cambias y luego a tuple (tuple es inmutable)\n",
        "        list_buffer = list(self.buffer[position])\n",
        "        list_buffer[2] = PENALTY\n",
        "        self.buffer[position] = tuple(list_buffer)\n",
        "       \n",
        "  \n",
        "    #CONFLICTIVO, comprobar bien en test ejecución\n",
        "    # Leve penalizacion para disparo en casos cercanos a la muerte\n",
        "    def set_penalty_shoot(self, position, PENALTY=-5):\n",
        "        if self.buffer[position][1] == 1:\n",
        "            list_buffer = list(self.buffer[position])\n",
        "            list_buffer[2] = PENALTY\n",
        "            self.buffer[position] = tuple(list_buffer)      \n",
        "        \n",
        "\n",
        "    def add_state(self, state):\n",
        "        \"\"\"Add an experience to the buffer\"\"\"\n",
        "        # S represents current state, a is action,\n",
        "        # and s2 is next state\n",
        "        if self.count < self.buffer_size:\n",
        "            self.buffer.append(state)\n",
        "            self.count += 1\n",
        "        else:\n",
        "            self.buffer.popleft()\n",
        "            self.buffer.append(s)\n",
        "\n",
        "    def size(self):\n",
        "        return self.count\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Samples a total of elements equal to batch_size from buffer\n",
        "        if buffer contains enough elements. Otherwise return all elements\"\"\"\n",
        "\n",
        "        batch = []\n",
        "\n",
        "        if self.count < batch_size:\n",
        "            batch = random.sample(self.buffer, self.count)\n",
        "        else:\n",
        "            batch = random.sample(self.buffer, batch_size)\n",
        "\n",
        "        # Maps each experience in batch in batches of states, actions, rewards\n",
        "        # and new states\n",
        "        s_batch, a_batch, r_batch, d_batch, s2_batch = list(map(np.array, list(zip(*batch))))\n",
        "\n",
        "        return s_batch, a_batch, r_batch, d_batch, s2_batch\n",
        "    def sample2(self):\n",
        "        buffer_size = len(self.buffer)\n",
        "        index = np.random.randint(0,buffer_size)\n",
        "        #index = np.random.choice(np.arange(buffer_size), size=1, replace=False)\n",
        "        \n",
        "    #return [self.buffer[i] for i in index]\n",
        "    #return random value of index\n",
        "        return self.buffer[index]\n",
        "\n",
        "    def get(self, element):\n",
        "        return self.buffer[element]\n",
        "\n",
        "    def get_state(self, element):\n",
        "        return self.buffer[element][0]\n",
        "\n",
        "\n",
        "    def clear(self):\n",
        "        self.buffer.clear()\n",
        "        self.count = 0\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85uk7CxrjfoA",
        "outputId": "cdea6dfb-8aba-4240-b586-a4d009190124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model, Sequential\n",
        "from keras.utils import CustomObjectScope\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.layers.convolutional import Convolution2D, Conv2D, Conv3D\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.core import Activation, Dropout, Flatten, Dense\n",
        "from keras.layers import TimeDistributed, LSTM, MaxPooling2D, ConvLSTM2D\n",
        "\n",
        "# List of hyper-parameters and constants\n",
        "DECAY_RATE = 0.99\n",
        "BUFFER_SIZE = 40000\n",
        "MINIBATCH_SIZE = 64\n",
        "TOT_FRAME = 3000000\n",
        "EPSILON_DECAY = 1000000\n",
        "MIN_OBSERVATION = 5000\n",
        "FINAL_EPSILON = 0.05\n",
        "INITIAL_EPSILON = 0.1\n",
        "NUM_ACTIONS = 4\n",
        "TAU = 0.01\n",
        "# Number of frames to throw into network\n",
        "NUM_FRAMES = 3\n",
        "\n",
        "class DeepQ(object):\n",
        "    \"\"\"Constructs the desired deep q learning network\"\"\"\n",
        "    def __init__(self):\n",
        "        self.construct_q_network()\n",
        "\n",
        "    def construct_q_network(self):\n",
        "         # Uses the network architecture found in DeepMind paper\n",
        "        # define CNN model\n",
        "        self.model = tf.keras.Sequential()\n",
        "\n",
        "        self.model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(16,kernel_size=8,strides=4,activation='relu'), input_shape=(NUM_FRAMES,84,84,1)))\n",
        "\n",
        "        self.model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(32,kernel_size=4,strides=2,activation='relu')))\n",
        "\n",
        "        self.model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(32,kernel_size=3, strides=1,activation='relu')))\n",
        "\n",
        "        self.model.add(tf.keras.layers.ConvLSTM2D(64, kernel_size=(3,3), padding='same', return_sequences=False))\n",
        "\n",
        "        self.model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten()))\n",
        "\n",
        "        self.model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "\n",
        "        self.model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "        self.model.add(tf.keras.layers.Dense(4, activation='relu'))\n",
        "\n",
        "        self.model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=0.0001))\n",
        "\n",
        "        self.model.summary()\n",
        "\n",
        "        # Creates a target network as described in DeepMind paper\n",
        "        self.target_model = tf.keras.Sequential()\n",
        "\n",
        "        self.target_model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(16,kernel_size=8,strides=4,activation='relu'),input_shape=(NUM_FRAMES,84,84,1)))\n",
        "\n",
        "        self.target_model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(32,kernel_size=4,strides=2,activation='relu')))\n",
        "\n",
        "        self.target_model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(32,kernel_size=3, strides=1,activation='relu')))\n",
        "\n",
        "        self.target_model.add(tf.keras.layers.ConvLSTM2D(64, kernel_size=(3,3), padding='same', return_sequences=False))\n",
        "\n",
        "        self.target_model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten()))\n",
        "\n",
        "        self.target_model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "\n",
        "        self.target_model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "        self.target_model.add(tf.keras.layers.Dense(4, activation='relu'))\n",
        "\n",
        "        self.target_model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=0.0001))\n",
        "\n",
        "        self.target_model.summary()\n",
        "\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "        print(\"Successfully constructed networks.\")\n",
        "\n",
        "    def predict_movement(self, data, epsilon):\n",
        "        \"\"\"Predict movement of game controler where is epsilon\n",
        "        probability randomly move.\"\"\"\n",
        "        float_data = np.array(data).astype(np.float32)\n",
        "        q_actions = self.model.predict(float_data.reshape(1, NUM_FRAMES, 84, 84, 1))\n",
        "        #print(q_actions)\n",
        "        opt_policy = np.argmax(q_actions)\n",
        "        rand_val = np.random.random()\n",
        "        if rand_val < epsilon:\n",
        "            opt_policy = np.random.randint(0, NUM_ACTIONS)\n",
        "\n",
        "        #print(q_actions)\n",
        "        return opt_policy\n",
        "\n",
        "    def train(self, s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num):\n",
        "        \"\"\"Trains network to fit given parameters\"\"\"\n",
        "        batch_size = s_batch.shape[0]\n",
        "        #print(\"baia\", batch_size)\n",
        "        targets = np.zeros((batch_size, NUM_ACTIONS))\n",
        "        #print(\"PERO BUENO\", s_batch.shape)\n",
        "        states = np.zeros(((((32,NUM_FRAMES,84,84,1)))))\n",
        "        future_states = np.zeros(((((32,NUM_FRAMES,84,84,1)))))\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            states[i] = np.array(s_batch[i]).astype(np.float32)\n",
        "            #print(\"STATES\", states)\n",
        "            future_states[i] = np.array(s2_batch[i]).astype(np.float32)\n",
        "            targets[i] = self.model.predict(states[i].reshape(1, NUM_FRAMES, 84, 84 , 1), batch_size = 1)\n",
        "            fut_action = self.target_model.predict(future_states[i].reshape(1, NUM_FRAMES, 84, 84, 1), batch_size = 1)\n",
        "            targets[i, a_batch[i]] = r_batch[i]\n",
        "            if d_batch[i] == False:\n",
        "                targets[i, a_batch[i]] += DECAY_RATE * np.max(fut_action)\n",
        "            #print(targets)\n",
        "\n",
        "        loss = self.model.train_on_batch(states, targets)\n",
        "\n",
        "        # Print the loss every 10 iterations.\n",
        "        if observation_num % 10 == 0:\n",
        "            print(\"We had a loss equal to \", loss)\n",
        "\n",
        "    def save_network(self):\n",
        "        # Saves model at specified path as h5 file\n",
        "        self.model.save('saved.h5')\n",
        "        model_file = drive.CreateFile({'title' : 'saved.h5'}) \n",
        "        model_file.SetContentFile('saved.h5')\n",
        "        model_file.Upload()\n",
        "\n",
        "        drive.CreateFile({'id': model_file.get('id')})\n",
        "\n",
        "        \n",
        "        \"\"\"path = F\"/gdrive/My \\Drive/Modelos/saved.h5\" \n",
        "        self.model.save(path)\n",
        "        print(\"Successfully saved network.\")\"\"\"\n",
        "\n",
        "    def load_network(self):\n",
        "      with CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n",
        "        self.model = load_model('saved.h5')\n",
        "        \n",
        "        \n",
        "        #file_obj = drive.CreateFile({'id': '16zbkEN4vqnPvIOvLMF1IaXKYCq5g3Yln'})\n",
        "        #file_obj.GetContentFile('saved.h5')\n",
        "        #model = keras.models.load_model('saved.h5')\n",
        "        \n",
        "        \"\"\"#model_save_name = 'saved.h5'\n",
        "        path = F\"/content/gdrive/My \\Drive/Modelos/saved.h5\" \n",
        "        self.model = load_model(path)\"\"\"\n",
        "        print(\"Succesfully loaded network.\")\n",
        "\n",
        "    def target_train(self):\n",
        "        model_weights = self.model.get_weights()\n",
        "        target_model_weights = self.target_model.get_weights()\n",
        "        for i in range(len(model_weights)):\n",
        "            target_model_weights[i] = TAU * model_weights[i] + (1 - TAU) * target_model_weights[i]\n",
        "        self.target_model.set_weights(target_model_weights)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Haven't finished implementing yet...'\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Haven't finished implementing yet...'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py4SMXQejjki"
      },
      "source": [
        "import sys\n",
        "import gym\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# List of hyper-parameters and constants\n",
        "BUFFER_SIZE = 100000\n",
        "MINIBATCH_SIZE = 32\n",
        "TOT_FRAME = 1000000\n",
        "EPSILON_DECAY = 300000\n",
        "MIN_OBSERVATION = 5000\n",
        "FINAL_EPSILON = 0.1\n",
        "INITIAL_EPSILON = 1.0\n",
        "# Number of frames to throw into network\n",
        "NUM_FRAMES = 3\n",
        "\n",
        "class SpaceInvader(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.env = gym.make('SpaceInvaders-v0')\n",
        "        self.env.reset()\n",
        "        self.replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
        "\n",
        "        # Construct appropriate network based on flags\n",
        "        self.deep_q = DeepQ()\n",
        "        self.deep_q.save_network()\n",
        "        #self.deep_q.save_network()\n",
        "        #self.deep_q.load_network()\n",
        "        # A buffer that keeps the last 3 images\n",
        "        self.process_buffer = []\n",
        "        # Initialize buffer with the first frame\n",
        "        \"\"\"s1, r1, _, _ = self.env.step(0)\n",
        "        s2, r2, _, _ = self.env.step(0)\n",
        "        s3, r3, _, _ = self.env.step(0)\n",
        "        self.process_buffer = [s1, s2, s3]\"\"\"\n",
        "\n",
        "    def load_network(self):\n",
        "        self.deep_q.load_network()\n",
        "\n",
        "    def convert_process_buffer(self):\n",
        "        \"\"\"Converts the list of NUM_FRAMES images in the process buffer\n",
        "        into one training sample\"\"\"\n",
        "        black_buffer = [cv2.resize(cv2.cvtColor(x, cv2.COLOR_RGB2GRAY), (84, 90)) for x in self.process_buffer]\n",
        "        black_buffer = [x[1:85, :, np.newaxis] for x in black_buffer]\n",
        "        return np.concatenate(black_buffer, axis=2)\n",
        "\n",
        "    def preprocess_state(self, state):\n",
        "        \n",
        "        \"\"\"Converts the list of NUM_FRAMES images in the process buffer\n",
        "        into one training sample\"\"\"\n",
        "        black_buffer = cv2.resize(cv2.cvtColor(state, cv2.COLOR_RGB2GRAY), (84, 90))\n",
        "        black_buffer = [black_buffer[1:85, :, np.newaxis]]\n",
        "\n",
        "        return np.concatenate(black_buffer, axis=2)\n",
        "\n",
        "    def train(self, num_frames):\n",
        "        observation_num = 0\n",
        "        epsilon = INITIAL_EPSILON\n",
        "        alive_frame = 0\n",
        "        total_reward = 0\n",
        "        for i in range(NUM_FRAMES):\n",
        "            s0, r0, _, _ = self.env.step(0)\n",
        "            s1 = self.preprocess_state(s0)\n",
        "            self.process_buffer.append(s1)\n",
        "\n",
        "        new_state = np.array([np.array(xi) for xi in self.process_buffer])\n",
        "\n",
        "\n",
        "        while observation_num < num_frames:\n",
        "            if observation_num % 1000 == 999:\n",
        "                print((\"Executing loop %d\" % (observation_num)))\n",
        "\n",
        "            # Slowly decay the learning rate\n",
        "            if epsilon > FINAL_EPSILON:\n",
        "                epsilon -= (INITIAL_EPSILON-FINAL_EPSILON)/EPSILON_DECAY\n",
        "\n",
        "            initial_state = new_state\n",
        "            self.process_buffer = []\n",
        "\n",
        "            predict_movement = self.deep_q.predict_movement(initial_state, epsilon)\n",
        "\n",
        "\n",
        "\n",
        "            reward, done = 0, False\n",
        "            for i in range(NUM_FRAMES):\n",
        "                temp_observation, temp_reward, temp_done, _ = self.env.step(predict_movement)\n",
        "                tf_observation = self.preprocess_state(temp_observation)\n",
        "                reward += temp_reward\n",
        "                self.process_buffer.append(tf_observation)\n",
        "                done = done | temp_done\n",
        "\n",
        "            #self.process_numpy_buffer = np.array([np.array(xi) for xi in self.process_buffer])                \n",
        "\n",
        "            #if observation_num % 10 == 0:\n",
        "            #print(\"We predicted a q value of \", )\n",
        "\n",
        "            if done:\n",
        "                print(\"Lived with maximum time \", alive_frame)\n",
        "                print(\"Earned a total of reward equal to \", total_reward)\n",
        "                self.env.reset()\n",
        "                alive_frame = 0\n",
        "                total_reward = 0\n",
        "\n",
        "            #new_state = self.convert_process_buffer()\n",
        "            new_state = np.array([np.array(xi) for xi in self.process_buffer])\n",
        "            #print(\"Pollas\", new_state)\n",
        "            #print(\"En vinagre\", new_state.shape)\n",
        "            self.replay_buffer.add(initial_state, predict_movement, reward, done, new_state)\n",
        "            total_reward += reward\n",
        "\n",
        "            if self.replay_buffer.size() > MIN_OBSERVATION:\n",
        "                s_batch, a_batch, r_batch, d_batch, s2_batch = self.replay_buffer.sample(MINIBATCH_SIZE)\n",
        "                self.deep_q.train(s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num)\n",
        "                self.deep_q.target_train()\n",
        "\n",
        "            # Save the network every 1000 iterations\n",
        "            if observation_num % 10000 == 9999:\n",
        "                print(\"Saving Network\")\n",
        "                self.deep_q.save_network()\n",
        "\n",
        "            alive_frame += 1\n",
        "            observation_num += 1\n",
        "\n",
        "    def calculate_mean(self, num_samples = 100):\n",
        "        reward_list = []\n",
        "        print(\"Printing scores of each trial\")\n",
        "        for i in range(NUM_FRAMES):\n",
        "                s0, r0, _, _ = self.env.step(0)\n",
        "                s1 = self.preprocess_state(s0)\n",
        "                self.process_buffer.append(s1)\n",
        "        new_state = np.array([np.array(xi) for xi in self.process_buffer])\n",
        "        \n",
        "\n",
        "        for i in range(num_samples):\n",
        "            done = False\n",
        "            tot_award = 0\n",
        "            self.env.reset()\n",
        "            while not done:\n",
        "                state = new_state\n",
        "                predict_movement = self.deep_q.predict_movement(state, 0.0)\n",
        "                observation, reward, done, _ = self.env.step(predict_movement)\n",
        "                tf_observation = self.preprocess_state(observation)\n",
        "                self.process_buffer.append(tf_observation)\n",
        "                self.process_buffer.pop(0)\n",
        "                new_state = np.array([np.array(xi) for xi in self.process_buffer])                \n",
        "                tot_award += reward\n",
        "                \n",
        "            print(tot_award)\n",
        "            reward_list.append(tot_award)\n",
        "        return np.mean(reward_list), np.std(reward_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaHvM61BjnUJ",
        "outputId": "24afa940-bd67-402f-b096-3c3af6c0b880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "game_instance = SpaceInvader()\n",
        "\n",
        "game_instance.train(100_000_000)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed_16 (TimeDis (None, 3, 20, 20, 16)     1040      \n",
            "_________________________________________________________________\n",
            "time_distributed_17 (TimeDis (None, 3, 9, 9, 32)       8224      \n",
            "_________________________________________________________________\n",
            "time_distributed_18 (TimeDis (None, 3, 7, 7, 32)       9248      \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_4 (ConvLSTM2D)  (None, 7, 7, 64)          221440    \n",
            "_________________________________________________________________\n",
            "time_distributed_19 (TimeDis (None, 7, 448)            0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 7, 16)             7184      \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 112)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 4)                 452       \n",
            "=================================================================\n",
            "Total params: 247,588\n",
            "Trainable params: 247,588\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed_20 (TimeDis (None, 3, 20, 20, 16)     1040      \n",
            "_________________________________________________________________\n",
            "time_distributed_21 (TimeDis (None, 3, 9, 9, 32)       8224      \n",
            "_________________________________________________________________\n",
            "time_distributed_22 (TimeDis (None, 3, 7, 7, 32)       9248      \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_5 (ConvLSTM2D)  (None, 7, 7, 64)          221440    \n",
            "_________________________________________________________________\n",
            "time_distributed_23 (TimeDis (None, 7, 448)            0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 7, 16)             7184      \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 112)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 4)                 452       \n",
            "=================================================================\n",
            "Total params: 247,588\n",
            "Trainable params: 247,588\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Successfully constructed networks.\n",
            "Lived with maximum time  265\n",
            "Earned a total of reward equal to  140.0\n",
            "Lived with maximum time  174\n",
            "Earned a total of reward equal to  20.0\n",
            "Lived with maximum time  311\n",
            "Earned a total of reward equal to  430.0\n",
            "Executing loop 999\n",
            "Lived with maximum time  299\n",
            "Earned a total of reward equal to  330.0\n",
            "Lived with maximum time  225\n",
            "Earned a total of reward equal to  110.0\n",
            "Lived with maximum time  305\n",
            "Earned a total of reward equal to  150.0\n",
            "Lived with maximum time  229\n",
            "Earned a total of reward equal to  135.0\n",
            "Executing loop 1999\n",
            "Lived with maximum time  237\n",
            "Earned a total of reward equal to  105.0\n",
            "Lived with maximum time  223\n",
            "Earned a total of reward equal to  35.0\n",
            "Lived with maximum time  199\n",
            "Earned a total of reward equal to  65.0\n",
            "Lived with maximum time  136\n",
            "Earned a total of reward equal to  20.0\n",
            "Lived with maximum time  177\n",
            "Earned a total of reward equal to  15.0\n",
            "Lived with maximum time  216\n",
            "Earned a total of reward equal to  85.0\n",
            "Executing loop 2999\n",
            "Lived with maximum time  215\n",
            "Earned a total of reward equal to  65.0\n",
            "Lived with maximum time  267\n",
            "Earned a total of reward equal to  315.0\n",
            "Lived with maximum time  152\n",
            "Earned a total of reward equal to  35.0\n",
            "Lived with maximum time  180\n",
            "Earned a total of reward equal to  60.0\n",
            "Executing loop 3999\n",
            "Lived with maximum time  206\n",
            "Earned a total of reward equal to  60.0\n",
            "Lived with maximum time  229\n",
            "Earned a total of reward equal to  20.0\n",
            "Lived with maximum time  199\n",
            "Earned a total of reward equal to  105.0\n",
            "Lived with maximum time  400\n",
            "Earned a total of reward equal to  235.0\n",
            "Executing loop 4999\n",
            "We had a loss equal to  0.05522697\n",
            "Lived with maximum time  166\n",
            "Earned a total of reward equal to  50.0\n",
            "We had a loss equal to  0.03726015\n",
            "We had a loss equal to  0.13639076\n",
            "We had a loss equal to  0.35432574\n",
            "We had a loss equal to  1.2082418\n",
            "We had a loss equal to  0.07132825\n",
            "We had a loss equal to  1.6909832\n",
            "We had a loss equal to  0.21237153\n",
            "We had a loss equal to  0.22435027\n",
            "We had a loss equal to  3.2671752\n",
            "We had a loss equal to  0.05075221\n",
            "We had a loss equal to  0.10563339\n",
            "We had a loss equal to  0.1154709\n",
            "We had a loss equal to  6.4305034\n",
            "We had a loss equal to  0.28773624\n",
            "We had a loss equal to  3.6072648\n",
            "We had a loss equal to  1.5711385\n",
            "We had a loss equal to  0.042932153\n",
            "We had a loss equal to  0.06714578\n",
            "We had a loss equal to  3.6568618\n",
            "We had a loss equal to  0.20982975\n",
            "We had a loss equal to  0.18763421\n",
            "We had a loss equal to  3.1462796\n",
            "We had a loss equal to  0.05691003\n",
            "We had a loss equal to  0.7612732\n",
            "We had a loss equal to  0.14448304\n",
            "We had a loss equal to  1.5152678\n",
            "We had a loss equal to  0.13716616\n",
            "We had a loss equal to  4.44106\n",
            "We had a loss equal to  0.16920745\n",
            "We had a loss equal to  0.25695205\n",
            "We had a loss equal to  2.9476\n",
            "Lived with maximum time  305\n",
            "Earned a total of reward equal to  90.0\n",
            "We had a loss equal to  0.14846505\n",
            "We had a loss equal to  0.16649264\n",
            "We had a loss equal to  0.3548721\n",
            "We had a loss equal to  7.1049123\n",
            "We had a loss equal to  0.6751826\n",
            "We had a loss equal to  0.9795071\n",
            "We had a loss equal to  0.76995033\n",
            "We had a loss equal to  0.7499042\n",
            "We had a loss equal to  0.8592164\n",
            "We had a loss equal to  0.13019338\n",
            "We had a loss equal to  1.8240657\n",
            "We had a loss equal to  1.9822514\n",
            "We had a loss equal to  0.86503863\n",
            "We had a loss equal to  1.2174498\n",
            "We had a loss equal to  0.08877818\n",
            "We had a loss equal to  5.988267\n",
            "We had a loss equal to  2.7185972\n",
            "Lived with maximum time  175\n",
            "Earned a total of reward equal to  120.0\n",
            "We had a loss equal to  3.0201628\n",
            "We had a loss equal to  0.14317498\n",
            "We had a loss equal to  0.29060507\n",
            "We had a loss equal to  7.453205\n",
            "We had a loss equal to  4.331829\n",
            "We had a loss equal to  0.2205255\n",
            "We had a loss equal to  4.112583\n",
            "We had a loss equal to  1.0111516\n",
            "We had a loss equal to  4.2866077\n",
            "We had a loss equal to  1.1779722\n",
            "We had a loss equal to  0.7306168\n",
            "We had a loss equal to  0.44046968\n",
            "We had a loss equal to  1.4778411\n",
            "We had a loss equal to  3.8990345\n",
            "Lived with maximum time  131\n",
            "Earned a total of reward equal to  10.0\n",
            "We had a loss equal to  0.29816818\n",
            "We had a loss equal to  1.8847213\n",
            "We had a loss equal to  0.2411757\n",
            "We had a loss equal to  1.7620795\n",
            "We had a loss equal to  8.682041\n",
            "We had a loss equal to  0.99696934\n",
            "We had a loss equal to  0.18934727\n",
            "We had a loss equal to  3.0578964\n",
            "We had a loss equal to  0.3749274\n",
            "We had a loss equal to  0.31372452\n",
            "We had a loss equal to  0.13536629\n",
            "We had a loss equal to  0.7099872\n",
            "We had a loss equal to  2.675569\n",
            "Lived with maximum time  135\n",
            "Earned a total of reward equal to  45.0\n",
            "We had a loss equal to  0.20619905\n",
            "We had a loss equal to  2.3813486\n",
            "We had a loss equal to  305.64542\n",
            "We had a loss equal to  1.7495108\n",
            "We had a loss equal to  0.31900406\n",
            "We had a loss equal to  0.43029323\n",
            "We had a loss equal to  0.8179695\n",
            "We had a loss equal to  4.232999\n",
            "We had a loss equal to  1.156884\n",
            "We had a loss equal to  0.18162474\n",
            "We had a loss equal to  0.19796303\n",
            "We had a loss equal to  0.38225833\n",
            "Lived with maximum time  124\n",
            "Earned a total of reward equal to  50.0\n",
            "We had a loss equal to  0.72903395\n",
            "We had a loss equal to  3.1736267\n",
            "We had a loss equal to  2.1396918\n",
            "We had a loss equal to  3.7937336\n",
            "We had a loss equal to  0.32649368\n",
            "We had a loss equal to  2.8330932\n",
            "We had a loss equal to  0.44727823\n",
            "We had a loss equal to  1.9837592\n",
            "We had a loss equal to  0.5598764\n",
            "We had a loss equal to  0.93163407\n",
            "We had a loss equal to  0.32795513\n",
            "We had a loss equal to  3.7323322\n",
            "Executing loop 5999\n",
            "We had a loss equal to  2.102695\n",
            "We had a loss equal to  0.73056567\n",
            "We had a loss equal to  0.29631957\n",
            "We had a loss equal to  1.9921876\n",
            "We had a loss equal to  1.3347325\n",
            "We had a loss equal to  0.20423678\n",
            "We had a loss equal to  0.15089568\n",
            "We had a loss equal to  3.2507079\n",
            "We had a loss equal to  1.1369277\n",
            "We had a loss equal to  0.9424119\n",
            "Lived with maximum time  218\n",
            "Earned a total of reward equal to  85.0\n",
            "We had a loss equal to  0.33945137\n",
            "We had a loss equal to  0.4654353\n",
            "We had a loss equal to  1.1113873\n",
            "We had a loss equal to  3.168555\n",
            "We had a loss equal to  0.34599757\n",
            "We had a loss equal to  2.3714266\n",
            "We had a loss equal to  0.1804483\n",
            "We had a loss equal to  0.70339304\n",
            "We had a loss equal to  2.7555432\n",
            "We had a loss equal to  0.74333\n",
            "We had a loss equal to  0.87966067\n",
            "We had a loss equal to  0.22914603\n",
            "We had a loss equal to  3.8420842\n",
            "We had a loss equal to  2.318883\n",
            "We had a loss equal to  0.41197693\n",
            "We had a loss equal to  0.2787225\n",
            "We had a loss equal to  18.854733\n",
            "Lived with maximum time  165\n",
            "Earned a total of reward equal to  50.0\n",
            "We had a loss equal to  0.28948748\n",
            "We had a loss equal to  0.42108625\n",
            "We had a loss equal to  0.9182035\n",
            "We had a loss equal to  0.6155256\n",
            "We had a loss equal to  0.34933257\n",
            "We had a loss equal to  6.470727\n",
            "We had a loss equal to  0.30823103\n",
            "We had a loss equal to  2.0303392\n",
            "We had a loss equal to  2.7923965\n",
            "We had a loss equal to  1.2202402\n",
            "We had a loss equal to  0.72620237\n",
            "We had a loss equal to  7.2943535\n",
            "We had a loss equal to  2.074305\n",
            "We had a loss equal to  1.9386438\n",
            "We had a loss equal to  0.6099046\n",
            "We had a loss equal to  3.3912177\n",
            "We had a loss equal to  0.71263605\n",
            "We had a loss equal to  1.7970097\n",
            "We had a loss equal to  4.820191\n",
            "We had a loss equal to  0.79347444\n",
            "We had a loss equal to  3.4987137\n",
            "We had a loss equal to  0.6723237\n",
            "We had a loss equal to  3.232722\n",
            "We had a loss equal to  9.695488\n",
            "We had a loss equal to  1.8748162\n",
            "We had a loss equal to  2.4610121\n",
            "We had a loss equal to  0.39403468\n",
            "We had a loss equal to  2.5393884\n",
            "We had a loss equal to  0.7759471\n",
            "We had a loss equal to  1.88404\n",
            "We had a loss equal to  1.5853068\n",
            "We had a loss equal to  8.031662\n",
            "We had a loss equal to  5.1278605\n",
            "We had a loss equal to  8.814809\n",
            "We had a loss equal to  1.0921471\n",
            "We had a loss equal to  5.5093117\n",
            "Lived with maximum time  362\n",
            "Earned a total of reward equal to  160.0\n",
            "We had a loss equal to  2.3564885\n",
            "We had a loss equal to  11.572643\n",
            "We had a loss equal to  0.856208\n",
            "We had a loss equal to  5.004916\n",
            "We had a loss equal to  0.94412947\n",
            "We had a loss equal to  4.7630577\n",
            "We had a loss equal to  1.1963315\n",
            "We had a loss equal to  1.3948301\n",
            "We had a loss equal to  0.78231\n",
            "We had a loss equal to  1.9247931\n",
            "We had a loss equal to  0.80775976\n",
            "We had a loss equal to  0.6043379\n",
            "We had a loss equal to  0.4342097\n",
            "We had a loss equal to  3.3537788\n",
            "We had a loss equal to  7.9834337\n",
            "We had a loss equal to  1.1826372\n",
            "We had a loss equal to  1.8049111\n",
            "We had a loss equal to  3.6076927\n",
            "We had a loss equal to  1.9635915\n",
            "We had a loss equal to  3.967556\n",
            "We had a loss equal to  2.220819\n",
            "We had a loss equal to  3.8043747\n",
            "We had a loss equal to  4.705975\n",
            "We had a loss equal to  0.55997336\n",
            "We had a loss equal to  0.4106683\n",
            "We had a loss equal to  0.8730687\n",
            "We had a loss equal to  5.1114874\n",
            "We had a loss equal to  8.135282\n",
            "We had a loss equal to  0.9456713\n",
            "Lived with maximum time  292\n",
            "Earned a total of reward equal to  205.0\n",
            "We had a loss equal to  1.432245\n",
            "We had a loss equal to  1.6046544\n",
            "We had a loss equal to  0.893219\n",
            "We had a loss equal to  0.6651627\n",
            "We had a loss equal to  0.65772915\n",
            "We had a loss equal to  5.382554\n",
            "We had a loss equal to  3.2171497\n",
            "We had a loss equal to  1.971996\n",
            "Executing loop 6999\n",
            "We had a loss equal to  4.3911\n",
            "We had a loss equal to  0.89721763\n",
            "We had a loss equal to  1.9850215\n",
            "We had a loss equal to  3.193513\n",
            "We had a loss equal to  1.2486703\n",
            "We had a loss equal to  257.54156\n",
            "We had a loss equal to  0.8728139\n",
            "We had a loss equal to  4.8273063\n",
            "We had a loss equal to  0.7882296\n",
            "We had a loss equal to  1.5410417\n",
            "We had a loss equal to  1.0887429\n",
            "We had a loss equal to  0.7065662\n",
            "We had a loss equal to  0.5424034\n",
            "Lived with maximum time  211\n",
            "Earned a total of reward equal to  80.0\n",
            "We had a loss equal to  3.0474067\n",
            "We had a loss equal to  2.59404\n",
            "We had a loss equal to  5.3748302\n",
            "We had a loss equal to  0.62439334\n",
            "We had a loss equal to  1.7686001\n",
            "We had a loss equal to  1.8444707\n",
            "We had a loss equal to  0.6255981\n",
            "We had a loss equal to  1.8701056\n",
            "We had a loss equal to  5.3001432\n",
            "We had a loss equal to  1.4580508\n",
            "We had a loss equal to  1.0485132\n",
            "We had a loss equal to  0.61513084\n",
            "We had a loss equal to  1.0585005\n",
            "Lived with maximum time  132\n",
            "Earned a total of reward equal to  50.0\n",
            "We had a loss equal to  2.5874028\n",
            "We had a loss equal to  3.3168411\n",
            "We had a loss equal to  2.5279224\n",
            "We had a loss equal to  1.1428046\n",
            "We had a loss equal to  1.6727176\n",
            "We had a loss equal to  3.9155924\n",
            "We had a loss equal to  2.7967277\n",
            "We had a loss equal to  9.29987\n",
            "We had a loss equal to  2.9554667\n",
            "We had a loss equal to  11.980789\n",
            "We had a loss equal to  8.0267725\n",
            "We had a loss equal to  5.5140944\n",
            "We had a loss equal to  12.497738\n",
            "We had a loss equal to  0.46731576\n",
            "We had a loss equal to  0.9914402\n",
            "We had a loss equal to  1.8873821\n",
            "We had a loss equal to  3.4818683\n",
            "We had a loss equal to  1.653322\n",
            "We had a loss equal to  10.206518\n",
            "We had a loss equal to  4.3891034\n",
            "We had a loss equal to  0.9297812\n",
            "We had a loss equal to  1.0676494\n",
            "Lived with maximum time  219\n",
            "Earned a total of reward equal to  75.0\n",
            "We had a loss equal to  1.3301799\n",
            "We had a loss equal to  1.8809941\n",
            "We had a loss equal to  8.451316\n",
            "We had a loss equal to  1.0917077\n",
            "We had a loss equal to  5.337118\n",
            "We had a loss equal to  0.9486677\n",
            "We had a loss equal to  3.867618\n",
            "We had a loss equal to  2.9646559\n",
            "We had a loss equal to  2.3395388\n",
            "We had a loss equal to  5.2983284\n",
            "We had a loss equal to  10.059923\n",
            "We had a loss equal to  0.47055393\n",
            "We had a loss equal to  1.712572\n",
            "We had a loss equal to  9.613919\n",
            "We had a loss equal to  2.024167\n",
            "We had a loss equal to  2.646649\n",
            "We had a loss equal to  1.5105844\n",
            "We had a loss equal to  11.792841\n",
            "We had a loss equal to  1.4149033\n",
            "We had a loss equal to  4.0882215\n",
            "We had a loss equal to  12.298777\n",
            "We had a loss equal to  1.2420413\n",
            "We had a loss equal to  0.6944357\n",
            "We had a loss equal to  5.450243\n",
            "We had a loss equal to  1.7317961\n",
            "We had a loss equal to  1.2984619\n",
            "We had a loss equal to  3.1910121\n",
            "We had a loss equal to  4.702474\n",
            "We had a loss equal to  1.8211529\n",
            "We had a loss equal to  0.8068619\n",
            "We had a loss equal to  1.5650995\n",
            "We had a loss equal to  0.9974375\n",
            "We had a loss equal to  6.6961527\n",
            "We had a loss equal to  1.4274225\n",
            "We had a loss equal to  2.567992\n",
            "We had a loss equal to  1.8012402\n",
            "We had a loss equal to  1.5415591\n",
            "We had a loss equal to  1.5620278\n",
            "We had a loss equal to  3.1559227\n",
            "We had a loss equal to  4.2429028\n",
            "We had a loss equal to  26.925941\n",
            "We had a loss equal to  2.932741\n",
            "Lived with maximum time  418\n",
            "Earned a total of reward equal to  385.0\n",
            "We had a loss equal to  4.3048706\n",
            "We had a loss equal to  4.1517334\n",
            "We had a loss equal to  3.2952237\n",
            "We had a loss equal to  4.287563\n",
            "We had a loss equal to  2.1952243\n",
            "We had a loss equal to  8.213888\n",
            "We had a loss equal to  8.18245\n",
            "We had a loss equal to  2.855774\n",
            "We had a loss equal to  8.759983\n",
            "We had a loss equal to  4.3868227\n",
            "Executing loop 7999\n",
            "We had a loss equal to  10.142523\n",
            "We had a loss equal to  1.6663774\n",
            "We had a loss equal to  2.0091033\n",
            "We had a loss equal to  243.89088\n",
            "We had a loss equal to  3.6796193\n",
            "We had a loss equal to  2.1398683\n",
            "We had a loss equal to  12.089716\n",
            "We had a loss equal to  7.2233524\n",
            "We had a loss equal to  5.0285416\n",
            "We had a loss equal to  1.7244308\n",
            "We had a loss equal to  1.3418596\n",
            "We had a loss equal to  16.894787\n",
            "We had a loss equal to  2.230801\n",
            "We had a loss equal to  7.7849884\n",
            "We had a loss equal to  3.8498034\n",
            "We had a loss equal to  1.4288559\n",
            "We had a loss equal to  5.6239653\n",
            "We had a loss equal to  1.9836426\n",
            "We had a loss equal to  8.057945\n",
            "We had a loss equal to  2.3300662\n",
            "We had a loss equal to  1.2076255\n",
            "We had a loss equal to  1.0277698\n",
            "We had a loss equal to  8.2977\n",
            "We had a loss equal to  3.4799461\n",
            "We had a loss equal to  2.9498417\n",
            "We had a loss equal to  3.0702758\n",
            "We had a loss equal to  10.09094\n",
            "We had a loss equal to  4.758108\n",
            "We had a loss equal to  266.4183\n",
            "Lived with maximum time  392\n",
            "Earned a total of reward equal to  155.0\n",
            "We had a loss equal to  7.640786\n",
            "We had a loss equal to  312.1004\n",
            "We had a loss equal to  1.859211\n",
            "We had a loss equal to  9.883669\n",
            "We had a loss equal to  8.038226\n",
            "We had a loss equal to  1.1615181\n",
            "We had a loss equal to  2.0280514\n",
            "We had a loss equal to  2.4388142\n",
            "We had a loss equal to  1.1164875\n",
            "We had a loss equal to  1.9355131\n",
            "We had a loss equal to  3.5800838\n",
            "We had a loss equal to  3.135819\n",
            "We had a loss equal to  8.593332\n",
            "We had a loss equal to  3.5688276\n",
            "We had a loss equal to  4.7277403\n",
            "We had a loss equal to  1.0360904\n",
            "We had a loss equal to  1.9057196\n",
            "We had a loss equal to  1.195286\n",
            "We had a loss equal to  2.9260924\n",
            "We had a loss equal to  16.343842\n",
            "We had a loss equal to  5.868065\n",
            "We had a loss equal to  1.528064\n",
            "We had a loss equal to  3.229089\n",
            "We had a loss equal to  3.432787\n",
            "We had a loss equal to  1.2664467\n",
            "We had a loss equal to  3.3428526\n",
            "We had a loss equal to  1.9284283\n",
            "We had a loss equal to  9.8315735\n",
            "We had a loss equal to  2.1934652\n",
            "We had a loss equal to  4.914251\n",
            "We had a loss equal to  17.26606\n",
            "Lived with maximum time  303\n",
            "Earned a total of reward equal to  240.0\n",
            "We had a loss equal to  13.616704\n",
            "We had a loss equal to  9.401182\n",
            "We had a loss equal to  6.2778316\n",
            "We had a loss equal to  8.56797\n",
            "We had a loss equal to  1.702096\n",
            "We had a loss equal to  7.1125827\n",
            "We had a loss equal to  1.7969987\n",
            "We had a loss equal to  1.4679174\n",
            "We had a loss equal to  4.3250613\n",
            "We had a loss equal to  4.404498\n",
            "We had a loss equal to  9.155138\n",
            "We had a loss equal to  1.3148131\n",
            "We had a loss equal to  4.30272\n",
            "We had a loss equal to  4.3821435\n",
            "We had a loss equal to  1.6643507\n",
            "We had a loss equal to  1.9184836\n",
            "We had a loss equal to  6.346491\n",
            "We had a loss equal to  0.73638856\n",
            "We had a loss equal to  2.9056678\n",
            "Lived with maximum time  189\n",
            "Earned a total of reward equal to  75.0\n",
            "We had a loss equal to  19.69844\n",
            "We had a loss equal to  6.4304714\n",
            "We had a loss equal to  7.0107365\n",
            "We had a loss equal to  0.87140745\n",
            "We had a loss equal to  1.804399\n",
            "We had a loss equal to  5.6205254\n",
            "We had a loss equal to  1.2367046\n",
            "We had a loss equal to  2.474964\n",
            "We had a loss equal to  2.5082443\n",
            "We had a loss equal to  25.70142\n",
            "We had a loss equal to  2.932362\n",
            "We had a loss equal to  24.6738\n",
            "We had a loss equal to  2.5483623\n",
            "We had a loss equal to  1.4255158\n",
            "We had a loss equal to  0.9832195\n",
            "We had a loss equal to  1.8778896\n",
            "We had a loss equal to  6.882357\n",
            "We had a loss equal to  1.1892108\n",
            "We had a loss equal to  3.029794\n",
            "We had a loss equal to  2.185352\n",
            "We had a loss equal to  1.7853762\n",
            "Executing loop 8999\n",
            "We had a loss equal to  1.1971381\n",
            "We had a loss equal to  13.658983\n",
            "We had a loss equal to  1.4429007\n",
            "We had a loss equal to  3.367233\n",
            "We had a loss equal to  0.8618592\n",
            "We had a loss equal to  4.719553\n",
            "We had a loss equal to  18.042883\n",
            "We had a loss equal to  3.2428842\n",
            "We had a loss equal to  6.9715\n",
            "We had a loss equal to  4.760519\n",
            "Lived with maximum time  316\n",
            "Earned a total of reward equal to  240.0\n",
            "We had a loss equal to  2.307253\n",
            "We had a loss equal to  14.765022\n",
            "We had a loss equal to  1.1062877\n",
            "We had a loss equal to  1.1289951\n",
            "We had a loss equal to  2.5496464\n",
            "We had a loss equal to  23.38528\n",
            "We had a loss equal to  3.2204552\n",
            "We had a loss equal to  36.25225\n",
            "We had a loss equal to  7.5276866\n",
            "We had a loss equal to  5.6425657\n",
            "We had a loss equal to  7.2210956\n",
            "We had a loss equal to  1.8592412\n",
            "We had a loss equal to  5.671111\n",
            "Lived with maximum time  127\n",
            "Earned a total of reward equal to  45.0\n",
            "We had a loss equal to  3.810185\n",
            "We had a loss equal to  2.4868026\n",
            "We had a loss equal to  4.406138\n",
            "We had a loss equal to  25.220457\n",
            "We had a loss equal to  5.7290874\n",
            "We had a loss equal to  17.252842\n",
            "We had a loss equal to  4.2950296\n",
            "We had a loss equal to  0.5959028\n",
            "We had a loss equal to  2.2975955\n",
            "We had a loss equal to  1.7746391\n",
            "We had a loss equal to  1.2852994\n",
            "We had a loss equal to  6.423917\n",
            "We had a loss equal to  10.904786\n",
            "We had a loss equal to  1.3500711\n",
            "We had a loss equal to  1.9706318\n",
            "We had a loss equal to  4.3945847\n",
            "We had a loss equal to  14.4942\n",
            "We had a loss equal to  2.1908088\n",
            "We had a loss equal to  9.802533\n",
            "We had a loss equal to  37.024433\n",
            "We had a loss equal to  6.56003\n",
            "We had a loss equal to  7.6664524\n",
            "We had a loss equal to  4.2312403\n",
            "We had a loss equal to  0.9427068\n",
            "We had a loss equal to  4.34167\n",
            "We had a loss equal to  1.9368\n",
            "We had a loss equal to  3.1278062\n",
            "We had a loss equal to  2.3201413\n",
            "We had a loss equal to  15.972078\n",
            "We had a loss equal to  1.7020826\n",
            "We had a loss equal to  6.864011\n",
            "Lived with maximum time  313\n",
            "Earned a total of reward equal to  190.0\n",
            "We had a loss equal to  0.59776455\n",
            "We had a loss equal to  3.7204635\n",
            "We had a loss equal to  1.5691487\n",
            "We had a loss equal to  2.7837272\n",
            "We had a loss equal to  3.63054\n",
            "We had a loss equal to  2.883245\n",
            "We had a loss equal to  7.128076\n",
            "We had a loss equal to  3.9922438\n",
            "We had a loss equal to  3.2567523\n",
            "We had a loss equal to  2.407693\n",
            "We had a loss equal to  20.237257\n",
            "We had a loss equal to  2.1972399\n",
            "We had a loss equal to  3.0861027\n",
            "We had a loss equal to  3.091396\n",
            "We had a loss equal to  25.988054\n",
            "We had a loss equal to  4.917201\n",
            "We had a loss equal to  4.518288\n",
            "We had a loss equal to  8.623815\n",
            "We had a loss equal to  10.592468\n",
            "We had a loss equal to  26.730482\n",
            "We had a loss equal to  9.090815\n",
            "We had a loss equal to  6.0627694\n",
            "We had a loss equal to  5.7633543\n",
            "We had a loss equal to  1.5698664\n",
            "We had a loss equal to  2.7343583\n",
            "We had a loss equal to  0.7917012\n",
            "We had a loss equal to  5.1250057\n",
            "We had a loss equal to  4.294312\n",
            "Lived with maximum time  283\n",
            "Earned a total of reward equal to  135.0\n",
            "We had a loss equal to  4.193575\n",
            "We had a loss equal to  252.31877\n",
            "We had a loss equal to  2.747223\n",
            "We had a loss equal to  5.589772\n",
            "We had a loss equal to  3.0714083\n",
            "We had a loss equal to  2.7819653\n",
            "We had a loss equal to  4.8409824\n",
            "We had a loss equal to  12.900446\n",
            "We had a loss equal to  3.7760782\n",
            "We had a loss equal to  1.9622731\n",
            "We had a loss equal to  15.602957\n",
            "We had a loss equal to  18.74431\n",
            "We had a loss equal to  2.4166803\n",
            "We had a loss equal to  4.529931\n",
            "We had a loss equal to  7.8602047\n",
            "We had a loss equal to  3.611955\n",
            "We had a loss equal to  1.8859029\n",
            "We had a loss equal to  2.614657\n",
            "Executing loop 9999\n",
            "Saving Network\n",
            "We had a loss equal to  3.710384\n",
            "We had a loss equal to  7.215609\n",
            "We had a loss equal to  2.3022907\n",
            "We had a loss equal to  2.1964536\n",
            "We had a loss equal to  1.4812865\n",
            "We had a loss equal to  5.5357265\n",
            "We had a loss equal to  6.8908606\n",
            "We had a loss equal to  2.2673016\n",
            "We had a loss equal to  10.719737\n",
            "Lived with maximum time  268\n",
            "Earned a total of reward equal to  80.0\n",
            "We had a loss equal to  28.678535\n",
            "We had a loss equal to  21.607414\n",
            "We had a loss equal to  2.321889\n",
            "We had a loss equal to  2.8033576\n",
            "We had a loss equal to  4.299697\n",
            "We had a loss equal to  36.74412\n",
            "We had a loss equal to  4.1713233\n",
            "We had a loss equal to  1.7849678\n",
            "We had a loss equal to  4.3171744\n",
            "We had a loss equal to  1.7296462\n",
            "We had a loss equal to  2.772869\n",
            "We had a loss equal to  1.5038726\n",
            "We had a loss equal to  38.190147\n",
            "We had a loss equal to  11.223988\n",
            "We had a loss equal to  3.5942934\n",
            "We had a loss equal to  7.632023\n",
            "We had a loss equal to  24.551731\n",
            "We had a loss equal to  53.371586\n",
            "We had a loss equal to  46.94524\n",
            "We had a loss equal to  1.9496276\n",
            "We had a loss equal to  4.1096764\n",
            "We had a loss equal to  2.9342206\n",
            "We had a loss equal to  5.429115\n",
            "We had a loss equal to  2.4095712\n",
            "We had a loss equal to  1.1318616\n",
            "We had a loss equal to  29.100065\n",
            "We had a loss equal to  8.460771\n",
            "Lived with maximum time  272\n",
            "Earned a total of reward equal to  155.0\n",
            "We had a loss equal to  2.8085833\n",
            "We had a loss equal to  18.407913\n",
            "We had a loss equal to  8.407236\n",
            "We had a loss equal to  1.1184223\n",
            "We had a loss equal to  14.629542\n",
            "We had a loss equal to  5.596446\n",
            "We had a loss equal to  5.885396\n",
            "We had a loss equal to  24.444204\n",
            "We had a loss equal to  2.7184267\n",
            "We had a loss equal to  4.556405\n",
            "We had a loss equal to  2.0362442\n",
            "We had a loss equal to  2.7895246\n",
            "We had a loss equal to  1.5302687\n",
            "We had a loss equal to  3.963838\n",
            "We had a loss equal to  15.363304\n",
            "We had a loss equal to  27.235264\n",
            "We had a loss equal to  22.22497\n",
            "We had a loss equal to  8.315498\n",
            "We had a loss equal to  12.494087\n",
            "We had a loss equal to  28.313854\n",
            "We had a loss equal to  6.8727417\n",
            "Lived with maximum time  210\n",
            "Earned a total of reward equal to  80.0\n",
            "We had a loss equal to  31.835995\n",
            "We had a loss equal to  5.7558002\n",
            "We had a loss equal to  3.3033671\n",
            "We had a loss equal to  2.317058\n",
            "We had a loss equal to  2.055905\n",
            "We had a loss equal to  1.843541\n",
            "We had a loss equal to  6.454658\n",
            "We had a loss equal to  3.432551\n",
            "We had a loss equal to  222.12036\n",
            "We had a loss equal to  3.0390038\n",
            "We had a loss equal to  26.450903\n",
            "We had a loss equal to  10.906022\n",
            "We had a loss equal to  3.6036448\n",
            "We had a loss equal to  4.286324\n",
            "We had a loss equal to  6.543974\n",
            "Lived with maximum time  147\n",
            "Earned a total of reward equal to  20.0\n",
            "We had a loss equal to  9.617298\n",
            "We had a loss equal to  2.587248\n",
            "We had a loss equal to  2.0395637\n",
            "We had a loss equal to  3.2628517\n",
            "We had a loss equal to  5.4448137\n",
            "We had a loss equal to  4.3490324\n",
            "We had a loss equal to  8.148247\n",
            "We had a loss equal to  4.8068852\n",
            "We had a loss equal to  6.227383\n",
            "We had a loss equal to  4.779869\n",
            "We had a loss equal to  5.7372875\n",
            "We had a loss equal to  4.683546\n",
            "We had a loss equal to  2.304874\n",
            "We had a loss equal to  5.1649323\n",
            "We had a loss equal to  4.525921\n",
            "We had a loss equal to  6.2629447\n",
            "We had a loss equal to  13.495788\n",
            "We had a loss equal to  40.35556\n",
            "We had a loss equal to  2.4073446\n",
            "We had a loss equal to  8.026089\n",
            "We had a loss equal to  1.86076\n",
            "We had a loss equal to  3.5834105\n",
            "We had a loss equal to  2.0112455\n",
            "We had a loss equal to  16.709188\n",
            "We had a loss equal to  18.17881\n",
            "We had a loss equal to  13.324675\n",
            "We had a loss equal to  3.0145884\n",
            "We had a loss equal to  24.802942\n",
            "Lived with maximum time  278\n",
            "Earned a total of reward equal to  125.0\n",
            "Executing loop 10999\n",
            "We had a loss equal to  3.9547884\n",
            "We had a loss equal to  3.5031729\n",
            "We had a loss equal to  3.216634\n",
            "We had a loss equal to  237.2769\n",
            "We had a loss equal to  19.50877\n",
            "We had a loss equal to  2.692052\n",
            "We had a loss equal to  3.8192909\n",
            "We had a loss equal to  4.3238244\n",
            "We had a loss equal to  2.7215343\n",
            "We had a loss equal to  11.483114\n",
            "We had a loss equal to  2.2583847\n",
            "We had a loss equal to  3.9412847\n",
            "We had a loss equal to  1.9121854\n",
            "We had a loss equal to  2.7929583\n",
            "We had a loss equal to  5.2874084\n",
            "We had a loss equal to  5.047761\n",
            "We had a loss equal to  8.150541\n",
            "Lived with maximum time  170\n",
            "Earned a total of reward equal to  40.0\n",
            "We had a loss equal to  1.7011046\n",
            "We had a loss equal to  8.690272\n",
            "We had a loss equal to  14.812693\n",
            "We had a loss equal to  6.268066\n",
            "We had a loss equal to  3.5230646\n",
            "We had a loss equal to  4.479979\n",
            "We had a loss equal to  26.984377\n",
            "We had a loss equal to  3.2326198\n",
            "We had a loss equal to  9.291904\n",
            "We had a loss equal to  7.4381356\n",
            "We had a loss equal to  1.3361053\n",
            "We had a loss equal to  6.968621\n",
            "We had a loss equal to  3.0358539\n",
            "We had a loss equal to  10.674177\n",
            "We had a loss equal to  2.5803623\n",
            "We had a loss equal to  26.631123\n",
            "We had a loss equal to  2.6109\n",
            "We had a loss equal to  1.8181632\n",
            "Lived with maximum time  183\n",
            "Earned a total of reward equal to  105.0\n",
            "We had a loss equal to  3.9818563\n",
            "We had a loss equal to  9.568534\n",
            "We had a loss equal to  3.770372\n",
            "We had a loss equal to  44.132576\n",
            "We had a loss equal to  4.8274927\n",
            "We had a loss equal to  1.5519825\n",
            "We had a loss equal to  6.7843113\n",
            "We had a loss equal to  3.7089238\n",
            "We had a loss equal to  1.8511024\n",
            "We had a loss equal to  3.3795638\n",
            "We had a loss equal to  6.251337\n",
            "We had a loss equal to  30.611073\n",
            "We had a loss equal to  36.61946\n",
            "We had a loss equal to  11.55941\n",
            "We had a loss equal to  6.3986497\n",
            "We had a loss equal to  2.3807669\n",
            "Lived with maximum time  162\n",
            "Earned a total of reward equal to  40.0\n",
            "We had a loss equal to  5.166747\n",
            "We had a loss equal to  1.8775423\n",
            "We had a loss equal to  3.3667426\n",
            "We had a loss equal to  294.61264\n",
            "We had a loss equal to  2.333572\n",
            "We had a loss equal to  5.2423177\n",
            "We had a loss equal to  7.4933825\n",
            "We had a loss equal to  4.6754384\n",
            "We had a loss equal to  44.6325\n",
            "We had a loss equal to  2.1795893\n",
            "We had a loss equal to  2.1180606\n",
            "We had a loss equal to  3.9323025\n",
            "We had a loss equal to  2.9797616\n",
            "We had a loss equal to  2.186695\n",
            "We had a loss equal to  18.68394\n",
            "We had a loss equal to  6.331443\n",
            "We had a loss equal to  3.287218\n",
            "We had a loss equal to  7.971204\n",
            "We had a loss equal to  3.1012292\n",
            "We had a loss equal to  3.3938851\n",
            "We had a loss equal to  1.3672576\n",
            "We had a loss equal to  3.3557298\n",
            "We had a loss equal to  2.5722532\n",
            "We had a loss equal to  2.19304\n",
            "We had a loss equal to  2.0451334\n",
            "We had a loss equal to  6.5101547\n",
            "We had a loss equal to  5.576286\n",
            "We had a loss equal to  8.055107\n",
            "We had a loss equal to  41.46436\n",
            "We had a loss equal to  26.675446\n",
            "We had a loss equal to  5.0821977\n",
            "We had a loss equal to  87.24923\n",
            "We had a loss equal to  4.5829883\n",
            "We had a loss equal to  2.2749727\n",
            "We had a loss equal to  3.1970162\n",
            "We had a loss equal to  16.37658\n",
            "We had a loss equal to  70.1023\n",
            "Lived with maximum time  362\n",
            "Earned a total of reward equal to  160.0\n",
            "We had a loss equal to  4.7162046\n",
            "We had a loss equal to  1.6852715\n",
            "We had a loss equal to  3.2219641\n",
            "We had a loss equal to  4.3769317\n",
            "We had a loss equal to  13.34712\n",
            "We had a loss equal to  102.81093\n",
            "We had a loss equal to  3.9680862\n",
            "We had a loss equal to  3.7174973\n",
            "We had a loss equal to  4.904534\n",
            "We had a loss equal to  17.182877\n",
            "We had a loss equal to  3.5018003\n",
            "We had a loss equal to  4.845279\n",
            "Executing loop 11999\n",
            "We had a loss equal to  17.257486\n",
            "We had a loss equal to  1.8520783\n",
            "We had a loss equal to  6.7143345\n",
            "We had a loss equal to  3.136043\n",
            "We had a loss equal to  3.1828485\n",
            "We had a loss equal to  7.189678\n",
            "We had a loss equal to  5.184887\n",
            "We had a loss equal to  3.3799362\n",
            "Lived with maximum time  200\n",
            "Earned a total of reward equal to  80.0\n",
            "We had a loss equal to  3.5165758\n",
            "We had a loss equal to  7.3781805\n",
            "We had a loss equal to  2.6135712\n",
            "We had a loss equal to  16.740368\n",
            "We had a loss equal to  6.6878715\n",
            "We had a loss equal to  32.907707\n",
            "We had a loss equal to  9.415488\n",
            "We had a loss equal to  7.1873026\n",
            "We had a loss equal to  4.261143\n",
            "We had a loss equal to  4.3982954\n",
            "We had a loss equal to  2.4379203\n",
            "We had a loss equal to  28.330507\n",
            "We had a loss equal to  4.735449\n",
            "We had a loss equal to  1.7408819\n",
            "We had a loss equal to  4.1363277\n",
            "We had a loss equal to  25.023127\n",
            "We had a loss equal to  4.6982145\n",
            "Lived with maximum time  172\n",
            "Earned a total of reward equal to  50.0\n",
            "We had a loss equal to  3.0730348\n",
            "We had a loss equal to  5.1614356\n",
            "We had a loss equal to  4.786424\n",
            "We had a loss equal to  3.3533487\n",
            "We had a loss equal to  28.349041\n",
            "We had a loss equal to  2.8477454\n",
            "We had a loss equal to  5.2495375\n",
            "We had a loss equal to  34.52815\n",
            "We had a loss equal to  14.9908\n",
            "We had a loss equal to  2.7369173\n",
            "We had a loss equal to  3.15682\n",
            "We had a loss equal to  4.67093\n",
            "We had a loss equal to  1.1741469\n",
            "We had a loss equal to  4.8823433\n",
            "We had a loss equal to  4.1259165\n",
            "We had a loss equal to  3.7874541\n",
            "We had a loss equal to  3.645238\n",
            "We had a loss equal to  12.497122\n",
            "We had a loss equal to  1.2901554\n",
            "We had a loss equal to  2.9691567\n",
            "Lived with maximum time  198\n",
            "Earned a total of reward equal to  35.0\n",
            "We had a loss equal to  5.1087394\n",
            "We had a loss equal to  5.1467657\n",
            "We had a loss equal to  8.335352\n",
            "We had a loss equal to  313.14316\n",
            "We had a loss equal to  5.547242\n",
            "We had a loss equal to  4.23252\n",
            "We had a loss equal to  8.412112\n",
            "We had a loss equal to  7.7127614\n",
            "We had a loss equal to  5.7512527\n",
            "We had a loss equal to  3.9759755\n",
            "We had a loss equal to  88.90835\n",
            "We had a loss equal to  4.1029224\n",
            "We had a loss equal to  4.5300384\n",
            "We had a loss equal to  4.8366537\n",
            "We had a loss equal to  5.45585\n",
            "We had a loss equal to  5.0311413\n",
            "We had a loss equal to  2.9353037\n",
            "We had a loss equal to  10.484205\n",
            "We had a loss equal to  5.2636523\n",
            "We had a loss equal to  8.356625\n",
            "Lived with maximum time  203\n",
            "Earned a total of reward equal to  35.0\n",
            "We had a loss equal to  10.9669695\n",
            "We had a loss equal to  8.060695\n",
            "We had a loss equal to  5.9436526\n",
            "We had a loss equal to  5.3177457\n",
            "We had a loss equal to  6.69231\n",
            "We had a loss equal to  4.080081\n",
            "We had a loss equal to  5.347482\n",
            "We had a loss equal to  2.3615391\n",
            "We had a loss equal to  7.5891485\n",
            "We had a loss equal to  44.970844\n",
            "We had a loss equal to  3.307233\n",
            "We had a loss equal to  2.794123\n",
            "We had a loss equal to  7.091856\n",
            "We had a loss equal to  9.169085\n",
            "We had a loss equal to  4.699333\n",
            "We had a loss equal to  16.258896\n",
            "We had a loss equal to  28.264265\n",
            "We had a loss equal to  1.7811909\n",
            "We had a loss equal to  3.2161934\n",
            "We had a loss equal to  2.148008\n",
            "We had a loss equal to  17.674723\n",
            "We had a loss equal to  8.45219\n",
            "We had a loss equal to  6.915402\n",
            "We had a loss equal to  16.138447\n",
            "We had a loss equal to  11.130751\n",
            "We had a loss equal to  8.199958\n",
            "We had a loss equal to  16.216682\n",
            "We had a loss equal to  3.0217085\n",
            "We had a loss equal to  7.8134794\n",
            "Lived with maximum time  292\n",
            "Earned a total of reward equal to  170.0\n",
            "We had a loss equal to  5.475646\n",
            "We had a loss equal to  23.212597\n",
            "We had a loss equal to  1.9563562\n",
            "We had a loss equal to  5.7157464\n",
            "We had a loss equal to  6.35291\n",
            "We had a loss equal to  4.0853405\n",
            "Executing loop 12999\n",
            "We had a loss equal to  4.447527\n",
            "We had a loss equal to  3.6323223\n",
            "We had a loss equal to  7.5540867\n",
            "We had a loss equal to  2.947358\n",
            "We had a loss equal to  4.047958\n",
            "We had a loss equal to  3.6869087\n",
            "We had a loss equal to  5.24222\n",
            "We had a loss equal to  45.37231\n",
            "We had a loss equal to  8.469693\n",
            "We had a loss equal to  5.92769\n",
            "We had a loss equal to  7.6845727\n",
            "We had a loss equal to  11.082486\n",
            "We had a loss equal to  5.253768\n",
            "We had a loss equal to  2.4413986\n",
            "We had a loss equal to  3.7374098\n",
            "We had a loss equal to  13.492098\n",
            "We had a loss equal to  3.4765377\n",
            "We had a loss equal to  6.2543626\n",
            "We had a loss equal to  5.1271\n",
            "We had a loss equal to  1.1126884\n",
            "We had a loss equal to  4.8891015\n",
            "We had a loss equal to  4.6182923\n",
            "We had a loss equal to  2.892386\n",
            "Lived with maximum time  291\n",
            "Earned a total of reward equal to  320.0\n",
            "We had a loss equal to  4.613729\n",
            "We had a loss equal to  3.708043\n",
            "We had a loss equal to  48.363216\n",
            "We had a loss equal to  47.397278\n",
            "We had a loss equal to  6.5150476\n",
            "We had a loss equal to  6.1459055\n",
            "We had a loss equal to  3.8836372\n",
            "We had a loss equal to  5.3130245\n",
            "We had a loss equal to  3.9752321\n",
            "We had a loss equal to  1.4594102\n",
            "We had a loss equal to  8.49056\n",
            "We had a loss equal to  5.8952036\n",
            "We had a loss equal to  21.356346\n",
            "We had a loss equal to  3.3228736\n",
            "We had a loss equal to  8.566374\n",
            "We had a loss equal to  10.207879\n",
            "We had a loss equal to  4.2435794\n",
            "We had a loss equal to  9.762495\n",
            "We had a loss equal to  4.8879685\n",
            "Lived with maximum time  190\n",
            "Earned a total of reward equal to  30.0\n",
            "We had a loss equal to  7.8909316\n",
            "We had a loss equal to  6.112734\n",
            "We had a loss equal to  11.358055\n",
            "We had a loss equal to  6.6643457\n",
            "We had a loss equal to  3.2029655\n",
            "We had a loss equal to  9.061758\n",
            "We had a loss equal to  14.146889\n",
            "We had a loss equal to  4.895096\n",
            "We had a loss equal to  2.9646127\n",
            "We had a loss equal to  3.477934\n",
            "We had a loss equal to  2.379027\n",
            "We had a loss equal to  14.288088\n",
            "We had a loss equal to  22.579872\n",
            "We had a loss equal to  3.4860134\n",
            "We had a loss equal to  3.8904207\n",
            "We had a loss equal to  6.6571927\n",
            "We had a loss equal to  8.134922\n",
            "Lived with maximum time  164\n",
            "Earned a total of reward equal to  35.0\n",
            "We had a loss equal to  8.738359\n",
            "We had a loss equal to  6.5725408\n",
            "We had a loss equal to  3.5584888\n",
            "We had a loss equal to  3.1285262\n",
            "We had a loss equal to  5.520632\n",
            "We had a loss equal to  6.470443\n",
            "We had a loss equal to  3.3272915\n",
            "We had a loss equal to  2.4754488\n",
            "We had a loss equal to  6.221917\n",
            "We had a loss equal to  3.5745158\n",
            "We had a loss equal to  6.173517\n",
            "We had a loss equal to  19.176283\n",
            "We had a loss equal to  4.396095\n",
            "Lived with maximum time  136\n",
            "Earned a total of reward equal to  25.0\n",
            "We had a loss equal to  7.564584\n",
            "We had a loss equal to  5.1987205\n",
            "We had a loss equal to  24.061848\n",
            "We had a loss equal to  2.7086694\n",
            "We had a loss equal to  7.5373273\n",
            "We had a loss equal to  4.851496\n",
            "We had a loss equal to  6.7306786\n",
            "We had a loss equal to  6.7245636\n",
            "We had a loss equal to  6.02972\n",
            "We had a loss equal to  28.137917\n",
            "We had a loss equal to  6.8372297\n",
            "We had a loss equal to  5.003236\n",
            "We had a loss equal to  2.6986952\n",
            "We had a loss equal to  4.4943132\n",
            "We had a loss equal to  5.5481215\n",
            "We had a loss equal to  9.820745\n",
            "We had a loss equal to  6.1804214\n",
            "We had a loss equal to  2.1602535\n",
            "We had a loss equal to  5.630766\n",
            "We had a loss equal to  6.48567\n",
            "Lived with maximum time  200\n",
            "Earned a total of reward equal to  120.0\n",
            "We had a loss equal to  9.048573\n",
            "We had a loss equal to  4.748458\n",
            "We had a loss equal to  10.716011\n",
            "We had a loss equal to  5.7175775\n",
            "We had a loss equal to  2.8503673\n",
            "We had a loss equal to  4.6560736\n",
            "We had a loss equal to  3.7636971\n",
            "We had a loss equal to  3.0984278\n",
            "Executing loop 13999\n",
            "We had a loss equal to  13.342595\n",
            "We had a loss equal to  16.422525\n",
            "We had a loss equal to  10.933254\n",
            "We had a loss equal to  11.4283905\n",
            "We had a loss equal to  11.398059\n",
            "We had a loss equal to  10.977871\n",
            "We had a loss equal to  295.19244\n",
            "We had a loss equal to  4.162884\n",
            "We had a loss equal to  5.0716996\n",
            "We had a loss equal to  3.9183936\n",
            "We had a loss equal to  5.2853723\n",
            "We had a loss equal to  4.501809\n",
            "We had a loss equal to  4.6985683\n",
            "We had a loss equal to  5.287359\n",
            "We had a loss equal to  9.487173\n",
            "We had a loss equal to  13.067575\n",
            "We had a loss equal to  10.551649\n",
            "We had a loss equal to  7.106597\n",
            "We had a loss equal to  4.6781635\n",
            "We had a loss equal to  5.2883596\n",
            "We had a loss equal to  1.2192854\n",
            "Lived with maximum time  283\n",
            "Earned a total of reward equal to  120.0\n",
            "We had a loss equal to  8.142257\n",
            "We had a loss equal to  2.1202583\n",
            "We had a loss equal to  4.960911\n",
            "We had a loss equal to  3.1247678\n",
            "We had a loss equal to  2.967805\n",
            "We had a loss equal to  2.3566284\n",
            "We had a loss equal to  7.774359\n",
            "We had a loss equal to  8.157041\n",
            "We had a loss equal to  55.722393\n",
            "We had a loss equal to  4.1206923\n",
            "We had a loss equal to  5.0742054\n",
            "We had a loss equal to  10.298531\n",
            "We had a loss equal to  42.825653\n",
            "We had a loss equal to  29.928534\n",
            "We had a loss equal to  7.5990667\n",
            "We had a loss equal to  4.1213093\n",
            "We had a loss equal to  8.800294\n",
            "We had a loss equal to  5.172396\n",
            "We had a loss equal to  5.5848384\n",
            "We had a loss equal to  2.643632\n",
            "We had a loss equal to  3.3853056\n",
            "We had a loss equal to  4.612586\n",
            "Lived with maximum time  228\n",
            "Earned a total of reward equal to  110.0\n",
            "We had a loss equal to  4.7062273\n",
            "We had a loss equal to  4.1141124\n",
            "We had a loss equal to  38.62887\n",
            "We had a loss equal to  3.398699\n",
            "We had a loss equal to  5.597487\n",
            "We had a loss equal to  4.6534753\n",
            "We had a loss equal to  3.7261696\n",
            "We had a loss equal to  4.022482\n",
            "We had a loss equal to  4.3277206\n",
            "We had a loss equal to  14.995968\n",
            "We had a loss equal to  3.7030573\n",
            "We had a loss equal to  5.2151628\n",
            "We had a loss equal to  3.4082353\n",
            "We had a loss equal to  11.382361\n",
            "We had a loss equal to  3.2027314\n",
            "We had a loss equal to  4.426055\n",
            "We had a loss equal to  2.81711\n",
            "We had a loss equal to  5.3242035\n",
            "We had a loss equal to  4.505725\n",
            "We had a loss equal to  2.5047646\n",
            "We had a loss equal to  9.862837\n",
            "We had a loss equal to  20.911917\n",
            "We had a loss equal to  9.992113\n",
            "We had a loss equal to  8.43776\n",
            "We had a loss equal to  7.3094916\n",
            "We had a loss equal to  2.7278662\n",
            "We had a loss equal to  9.698222\n",
            "Lived with maximum time  262\n",
            "Earned a total of reward equal to  135.0\n",
            "We had a loss equal to  6.424048\n",
            "We had a loss equal to  16.829662\n",
            "We had a loss equal to  9.480753\n",
            "We had a loss equal to  9.588517\n",
            "We had a loss equal to  2.7861934\n",
            "We had a loss equal to  4.788018\n",
            "We had a loss equal to  13.460968\n",
            "We had a loss equal to  6.5381804\n",
            "We had a loss equal to  63.588474\n",
            "We had a loss equal to  4.80564\n",
            "We had a loss equal to  6.6333423\n",
            "We had a loss equal to  14.291214\n",
            "We had a loss equal to  6.384039\n",
            "We had a loss equal to  5.089928\n",
            "Lived with maximum time  142\n",
            "Earned a total of reward equal to  35.0\n",
            "We had a loss equal to  7.703225\n",
            "We had a loss equal to  16.119057\n",
            "We had a loss equal to  12.793857\n",
            "We had a loss equal to  4.4375043\n",
            "We had a loss equal to  12.446396\n",
            "We had a loss equal to  3.6559799\n",
            "We had a loss equal to  4.6512766\n",
            "We had a loss equal to  75.055016\n",
            "We had a loss equal to  75.20197\n",
            "We had a loss equal to  49.700233\n",
            "We had a loss equal to  15.682636\n",
            "We had a loss equal to  9.933983\n",
            "We had a loss equal to  36.085136\n",
            "We had a loss equal to  13.857503\n",
            "We had a loss equal to  4.550458\n",
            "We had a loss equal to  19.332462\n",
            "Executing loop 14999\n",
            "We had a loss equal to  5.0083523\n",
            "We had a loss equal to  7.1261063\n",
            "We had a loss equal to  4.207875\n",
            "We had a loss equal to  4.5618434\n",
            "We had a loss equal to  3.8493223\n",
            "We had a loss equal to  4.0568104\n",
            "We had a loss equal to  6.814391\n",
            "We had a loss equal to  3.7265575\n",
            "We had a loss equal to  5.637714\n",
            "We had a loss equal to  5.3598723\n",
            "We had a loss equal to  3.3705683\n",
            "We had a loss equal to  3.7245078\n",
            "We had a loss equal to  3.039826\n",
            "We had a loss equal to  5.050171\n",
            "Lived with maximum time  307\n",
            "Earned a total of reward equal to  110.0\n",
            "We had a loss equal to  5.857369\n",
            "We had a loss equal to  12.048772\n",
            "We had a loss equal to  9.490897\n",
            "We had a loss equal to  12.312401\n",
            "We had a loss equal to  30.213821\n",
            "We had a loss equal to  4.715378\n",
            "We had a loss equal to  15.281841\n",
            "We had a loss equal to  7.9100046\n",
            "We had a loss equal to  6.9758883\n",
            "We had a loss equal to  277.9276\n",
            "We had a loss equal to  7.148592\n",
            "We had a loss equal to  4.963573\n",
            "We had a loss equal to  24.692047\n",
            "We had a loss equal to  4.042062\n",
            "We had a loss equal to  4.95022\n",
            "We had a loss equal to  5.0258727\n",
            "We had a loss equal to  9.980455\n",
            "We had a loss equal to  7.445632\n",
            "We had a loss equal to  11.195044\n",
            "We had a loss equal to  7.7016253\n",
            "We had a loss equal to  5.1869364\n",
            "We had a loss equal to  9.423258\n",
            "We had a loss equal to  11.985891\n",
            "We had a loss equal to  7.3383207\n",
            "We had a loss equal to  3.0170717\n",
            "We had a loss equal to  6.6770487\n",
            "We had a loss equal to  8.607031\n",
            "We had a loss equal to  72.92818\n",
            "We had a loss equal to  11.105346\n",
            "We had a loss equal to  4.0517616\n",
            "We had a loss equal to  8.916433\n",
            "We had a loss equal to  13.267918\n",
            "We had a loss equal to  9.046285\n",
            "We had a loss equal to  7.9130735\n",
            "Lived with maximum time  337\n",
            "Earned a total of reward equal to  90.0\n",
            "We had a loss equal to  10.161936\n",
            "We had a loss equal to  263.18253\n",
            "We had a loss equal to  53.40017\n",
            "We had a loss equal to  10.576152\n",
            "We had a loss equal to  11.959562\n",
            "We had a loss equal to  19.436157\n",
            "We had a loss equal to  13.93066\n",
            "We had a loss equal to  176.07556\n",
            "We had a loss equal to  6.8692503\n",
            "We had a loss equal to  7.3349404\n",
            "We had a loss equal to  8.189726\n",
            "We had a loss equal to  5.140211\n",
            "We had a loss equal to  13.562769\n",
            "We had a loss equal to  5.9937863\n",
            "We had a loss equal to  47.506874\n",
            "We had a loss equal to  10.495104\n",
            "We had a loss equal to  10.591303\n",
            "We had a loss equal to  9.063481\n",
            "We had a loss equal to  30.93985\n",
            "We had a loss equal to  41.64622\n",
            "We had a loss equal to  20.644764\n",
            "We had a loss equal to  31.182718\n",
            "Lived with maximum time  222\n",
            "Earned a total of reward equal to  55.0\n",
            "We had a loss equal to  19.654615\n",
            "We had a loss equal to  4.694889\n",
            "We had a loss equal to  9.248385\n",
            "We had a loss equal to  49.54396\n",
            "We had a loss equal to  4.849932\n",
            "We had a loss equal to  8.459242\n",
            "We had a loss equal to  12.289708\n",
            "We had a loss equal to  4.60308\n",
            "We had a loss equal to  64.84057\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-4090671c6770>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgame_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpaceInvader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgame_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100_000_000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-04c1aea54bc0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_frames)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mMIN_OBSERVATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0ms_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMINIBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeep_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeep_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-4bc05c63d0a9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;31m#print(targets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# Print the loss every 10 iterations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1015\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMUDnsWJly2n",
        "outputId": "382d2a8d-de1b-4c6f-c568-281fff2470de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "game_instance_mean = SpaceInvader()\n",
        "\n",
        "game_instance_mean.deep_q.load_network()\n",
        "\n",
        "game_instance.calculate_mean(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed_64 (TimeDis (None, 3, 20, 20, 16)     1040      \n",
            "_________________________________________________________________\n",
            "time_distributed_65 (TimeDis (None, 3, 9, 9, 32)       8224      \n",
            "_________________________________________________________________\n",
            "time_distributed_66 (TimeDis (None, 3, 7, 7, 32)       9248      \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_16 (ConvLSTM2D) (None, 7, 7, 64)          221440    \n",
            "_________________________________________________________________\n",
            "time_distributed_67 (TimeDis (None, 7, 448)            0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 7, 16)             7184      \n",
            "_________________________________________________________________\n",
            "flatten_33 (Flatten)         (None, 112)               0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 4)                 452       \n",
            "=================================================================\n",
            "Total params: 247,588\n",
            "Trainable params: 247,588\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed_68 (TimeDis (None, 3, 20, 20, 16)     1040      \n",
            "_________________________________________________________________\n",
            "time_distributed_69 (TimeDis (None, 3, 9, 9, 32)       8224      \n",
            "_________________________________________________________________\n",
            "time_distributed_70 (TimeDis (None, 3, 7, 7, 32)       9248      \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_17 (ConvLSTM2D) (None, 7, 7, 64)          221440    \n",
            "_________________________________________________________________\n",
            "time_distributed_71 (TimeDis (None, 7, 448)            0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 7, 16)             7184      \n",
            "_________________________________________________________________\n",
            "flatten_35 (Flatten)         (None, 112)               0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 4)                 452       \n",
            "=================================================================\n",
            "Total params: 247,588\n",
            "Trainable params: 247,588\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Successfully constructed networks.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-49d69b4416a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgame_instance_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpaceInvader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgame_instance_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeep_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgame_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-4cfe3e8d446f>\u001b[0m in \u001b[0;36mload_network\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'GlorotUniform'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mglorot_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'write'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36m_deserialize_model\u001b[0;34m(h5dict, custom_objects, compile)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    591\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    166\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 147\u001b[0;31m                                         list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_configs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             layer = layer_module.deserialize(conf,\n\u001b[0;32m--> 301\u001b[0;31m                                              custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    302\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbuild_input_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    166\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    147\u001b[0m                                         list(custom_objects.items())))\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/convolutional_recurrent.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/convolutional_recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, return_sequences, go_backwards, stateful, dropout, recurrent_dropout, **kwargs)\u001b[0m\n\u001b[1;32m    955\u001b[0m                                          \u001b[0mgo_backwards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgo_backwards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m                                          \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m                                          **kwargs)\n\u001b[0m\u001b[1;32m    958\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivity_regularizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/convolutional_recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cell, return_sequences, return_state, go_backwards, stateful, unroll, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m                                         \u001b[0mstateful\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                                         \u001b[0munroll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                                         **kwargs)\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mInputSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cell, return_sequences, return_state, go_backwards, stateful, unroll, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m                              \u001b[0;34m'(tuple of integers, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                              'one integer per RNN state).')\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Keyword argument not understood:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'time_major')"
          ]
        }
      ]
    }
  ]
}